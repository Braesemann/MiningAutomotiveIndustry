{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl Webpages to Create a Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.data.path.append(\"/Users/niklasstoehr/Libraries\");\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <------------!!!!!!!!!! Change Crawl Behaviour\n",
    "\n",
    "def get_links(url):\n",
    "\n",
    "    for i in url:\n",
    "        \n",
    "        r = requests.get(i, allow_redirects = True, timeout=5)                                # <------------!!!!!!!!!!\n",
    "        soup = BeautifulSoup(r.content, \"lxml\")\n",
    "        \n",
    "        hrefs = []\n",
    "\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            hrefs.append(a['href'])\n",
    "            \n",
    "        hrefs = list(set(hrefs))   # remove duplicate links from webpage\n",
    "        #print(hrefs)\n",
    "\n",
    "        links = []\n",
    "        for href in hrefs:\n",
    "            if (href.startswith(\"http\")): # inlude https://www.linkedin.com/company/11234128/     # <------------!!!!!!!!!!\n",
    "                links.append(href)\n",
    "                        \n",
    "            if(href.startswith(\"#\")): \n",
    "                if str(url[0])[-1] == \"#\":               # avoid domain//about (double backslash)\n",
    "                    url[0] = url[0][:-1]\n",
    "                    links.append(str(url[0]) + href)\n",
    "                else:                                   # do domain/about (one backslash)\n",
    "                    links.append(str(url[0]) + href)  \n",
    "            if(href.startswith(\"/\")):                    # include /ourwork\n",
    "                if str(url[0])[-1] == \"/\":               # avoid domain//about (double backslash)\n",
    "                    url[0] = url[0][:-1]\n",
    "                    links.append(str(url[0]) + href)\n",
    "                else:                                   # do domain/about (one backslash)\n",
    "                    links.append(str(url[0]) + href)   \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        #print(links)\n",
    "        return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breadth_search(domain_name, search_depth, url):\n",
    "    \n",
    "    ## write crawl statistics\n",
    "    crawl_txt = open('results/' + domain_name + '_crawl.csv', \"w\")\n",
    "    crawl_txt.write('level,pages' + '\\n')\n",
    "    \n",
    "    ## write to file and print\n",
    "    crawl_txt.write(str(0) + ',' + str(1) + '\\n')\n",
    "    \n",
    "    crawled = deque(url)  # nodes already used as starting points of crawl\n",
    "    next_queue = deque([url])\n",
    "    level = 0\n",
    "\n",
    "    while level < search_depth:\n",
    "        \n",
    "        print ('\\nlevel:', level ,'queue length:', len(next_queue))\n",
    "        queue = next_queue\n",
    "        next_queue = deque([])\n",
    "\n",
    "        while (len(queue) > 0):\n",
    "            \n",
    "            url = queue.popleft()\n",
    "            \n",
    "            try:\n",
    "                links = get_links(url)  # get links, maybe parse the result of last statement\n",
    "\n",
    "                ## write to txt file\n",
    "                new_page_count = 0\n",
    "                for e in (links):\n",
    "                    \n",
    "                    if e not in url:  # check if link directs back to parent\n",
    "\n",
    "                        #if (\"www.\" + domain_name) in e:  # check if link is still on main page of company         # <------------!!!!!!!!!!\n",
    "                        #if (domain_name) in e:  # check if link is still on main page of company         # <------------!!!!!!!!!!\n",
    " \n",
    "                            #if (e.count('/') < 4+2): # if more backslashes than 5, stop the procedure              # <------------!!!!!!!!!!\n",
    "\n",
    "                        file.write((str(url[0]) + '\\t' + str(e) + '\\n')) ## write to edges file --> save link\n",
    "\n",
    "                        #____________________________________________________________________________________\n",
    "\n",
    "                        if [e] not in next_queue: # check if node already in next queue\n",
    "\n",
    "                            if e not in crawled: # check if node already used to crawl as starting point (source)\n",
    "\n",
    "                                next_queue.append([e])\n",
    "                                crawled.append(e) # nodes already used as starting points of crawl\n",
    "                                new_page_count += 1\n",
    "\n",
    "                print('\\t ...added ', new_page_count, 'links to next queue\\t', len(queue), ' left in current queue...')\n",
    "\n",
    "            except:\n",
    "                print(\"! cannot reach webpage !\")\n",
    "                pass\n",
    "            \n",
    "        level += 1\n",
    "        print(\"\\t ...total crawled pages:\", len(crawled), \"\\n\")\n",
    "        \n",
    "        ## write to file and print\n",
    "        crawl_txt.write(str(level) + ',' + str(len(next_queue)) + '\\n')\n",
    "\n",
    "    crawl_txt.close()\n",
    "    print(\"\\n\\ncrawler terminated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"https://www.vw.com\"       -> no http hrefs, no #, depth 4+2, no redirects, no domain without www, redirect no\n",
    "#\"https://www.gm.com\"       -> no http hrefs, depth 12+2, redirects, no domain without www, no #, redirect yes\n",
    "#\"https://www.toyota.com\"   -> no http hrefs, no #, depth 3+2, www.domain, no redirects\n",
    "#https://www.hyundai.co.uk  (\"https://www.hyundai.com/worldwide/\") -> no http hrefs, no #, depth 3+2, no domain without www, no redirects\n",
    "\n",
    "url = \"https://www.volkswagen.com\"\n",
    "\n",
    "domain_name = \"vw\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "level: 0 queue length: 1\n",
      "\t ...added  21 links to next queue\t 0  left in current queue...\n",
      "\t ...total crawled pages: 22 \n",
      "\n",
      "\n",
      "level: 1 queue length: 21\n",
      "\t ...added  7 links to next queue\t 20  left in current queue...\n",
      "\t ...added  7 links to next queue\t 19  left in current queue...\n",
      "! cannot reach webpage !\n",
      "\t ...added  69 links to next queue\t 17  left in current queue...\n",
      "! cannot reach webpage !\n",
      "\t ...added  20 links to next queue\t 15  left in current queue...\n",
      "\t ...added  23 links to next queue\t 14  left in current queue...\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "! cannot reach webpage !\n",
      "\t ...total crawled pages: 148 \n",
      "\n",
      "\n",
      "\n",
      "crawler terminated\n"
     ]
    }
   ],
   "source": [
    "file = open('results/links.txt', 'w')\n",
    "search_depth = 2\n",
    "\n",
    "breadth_search(domain_name, search_depth, [url])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_seen = set() # holds lines already seen\n",
    "outfile_txt = open('results/' + domain_name + '_edges.txt', \"w\")\n",
    "outfile_csv = open('results/' + domain_name + '_edges.csv', \"w\")\n",
    "outfile_csv.write('source,target\\n')\n",
    "\n",
    "for line in open('results/links.txt', \"r\"):\n",
    "    if line not in lines_seen: # not a duplicate\n",
    "        \n",
    "        outfile_txt.write(line)\n",
    "        \n",
    "        csv_line = re.split(r'\\t+', line)\n",
    "        try:\n",
    "            outfile_csv.write(str(csv_line[0])+ ',' + str(csv_line[1]))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        lines_seen.add(line)\n",
    "\n",
    "## Close all CSVs\n",
    "outfile_txt.close()\n",
    "outfile_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Node List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list()\n",
    "\n",
    "## find unique nodes____________________________________________\n",
    "\n",
    "with open('results/' + domain_name + '_edges.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if (row['source'] not in nodes):\n",
    "            nodes.append(row['source'])\n",
    "        if (row['target'] not in nodes):\n",
    "            nodes.append(row['target'])\n",
    "            \n",
    "print(\"nodes:\", len(nodes))\n",
    "\n",
    "\n",
    "\n",
    "## write nodes in nodes.csv_______________________________________\n",
    "\n",
    "nodes_csv = open('results/' + domain_name + '_nodes.csv', \"w\")\n",
    "nodes_csv.write('node' + '\\n')\n",
    "            \n",
    "for node in nodes:\n",
    "    \n",
    "    nodes_csv.write(str(node) + '\\n')\n",
    "\n",
    "## close all CSVs\n",
    "nodes_csv.close()\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_analysis(url, topic_dict):\n",
    "                \n",
    "    # Start Page Analsis_________________________________________________________________\n",
    "    analysis_results = dict()\n",
    "    \n",
    "    try:  ## Webpage crawlable\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(html)\n",
    "            \n",
    "    \n",
    "        # Text Extraction _______________________________________________________________________\n",
    "\n",
    "        # kill all script and style elements\n",
    "        for script in soup([\"script\", \"style\", \"head\", \"title\", \"[document]\"]):\n",
    "            script.extract()    # rip it out\n",
    "\n",
    "\n",
    "\n",
    "        # get text\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        ## convert to lower case\n",
    "        text = text.lower()\n",
    "        ##exclude all text shorter than \"min_characters\"\"\n",
    "        content = \"\"\n",
    "        min_characters = 10\n",
    "        for line in iter(text.splitlines()):\n",
    "            if len(line) > min_characters:\n",
    "                content = content + line + \"\\n\"            \n",
    "\n",
    "\n",
    "        # Check Amount of Videos _____________________________________________________________\n",
    "        videos = len(soup.find_all('video', recursive=True))\n",
    "        analysis_results[\"videos\"] = videos\n",
    "\n",
    "\n",
    "\n",
    "        # Check Connections to Social Media\n",
    "        if \"facebook\" in content or \"youtube\" in content or \"instagram\" in content or \"linkedin\" in content or \"twitter\" in content:\n",
    "            analysis_results[\"social_media\"] = 1\n",
    "        else:\n",
    "            analysis_results[\"social_media\"] = 0\n",
    "\n",
    "\n",
    "\n",
    "        # Text Analysis _______________________________________________________________________\n",
    "\n",
    "        for topic in topic_dict.keys():\n",
    "\n",
    "            keyword_count = 0\n",
    "            keywords = topic_dict[topic].split(\",\")\n",
    "\n",
    "            for k in keywords:\n",
    "                keyword_count += sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(k), content))\n",
    "\n",
    "            analysis_results[topic] = keyword_count\n",
    "\n",
    "\n",
    "        # Sentiment Analysis _______________________________________________________________________\n",
    "        blob = TextBlob(content)\n",
    "        sentiment = round(blob.sentiment.polarity,4)\n",
    "        analysis_results[\"sentiment\"] = sentiment\n",
    "\n",
    "\n",
    "        # Language Analysis _______________________________________________________________________  \n",
    "        language = blob.detect_language()\n",
    "        analysis_results[\"language\"] = language\n",
    "\n",
    "\n",
    "        #print(analysis_results)\n",
    "\n",
    "        return analysis_results\n",
    "    \n",
    "    \n",
    "    \n",
    "    except: ## Webpage not crawlable\n",
    "        \n",
    "        print(\"could not crawl webpage\")\n",
    "        analysis_results[\"emobility\"] = 0\n",
    "        analysis_results[\"autonomous\"] = 0\n",
    "        analysis_results[\"ai\"] = 0\n",
    "        analysis_results[\"sentiment\"] = 0.0\n",
    "        analysis_results[\"language\"] = \"en\"\n",
    "        analysis_results[\"videos\"] = 0\n",
    "        analysis_results[\"social_media\"] = 0\n",
    "\n",
    "        if \"facebook\" in url or \"youtube\" in url or \"instagram\" in url or \"linkedin\" in url or \"twitter\" in url:\n",
    "            analysis_results[\"social_media\"] = 1\n",
    "\n",
    "        return analysis_results # 'exit' function and return to caller\n",
    "\n",
    "\n",
    "#webpage = \"https://www.audi.com/en.html\"\n",
    "#topic_results = topic_analysis(webpage, topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = dict()\n",
    "topic_dict[\"emobility\"] = \"emobility,battery,environment,bio,eco,ecological,electric,hybrid,environmental\"\n",
    "topic_dict[\"autonomous\"] = \"autonomous,self-driving\"\n",
    "topic_dict[\"ai\"] = \"ai,machine learning,artificial intelligence,intelligent,neural network,algorithm\"                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reader\n",
    "nodes_csv = open('results/' + domain_name + '_nodes.csv')\n",
    "reader = csv.DictReader(nodes_csv)\n",
    "\n",
    "## Writer\n",
    "nodes_tags_csv = open('results/' + domain_name + '_nodes_tags.csv', \"w\")\n",
    "nodes_tags_csv.write('id,label,videos,social_media,emobility,autonomous,ai,sentiment,language' + '\\n')\n",
    "\n",
    "## Do Analysis\n",
    "i = 0\n",
    "for row in reader:\n",
    "    print(\"\\n\", i ,\"of\", len(nodes))\n",
    "    webpage = row['node']\n",
    "\n",
    "    topic_results = topic_analysis(webpage, topic_dict)\n",
    "    print(topic_results)\n",
    "    nodes_tags_csv.write(str(webpage) + \",\" + str(webpage) + \",\" + str(topic_results['videos']) + \",\" + str(topic_results['social_media']) + \",\" + str(topic_results['emobility']) + \",\" + str(topic_results['autonomous']) + \",\" + str(topic_results['ai']) + \",\" + str(topic_results['sentiment']) + \",\" + str(topic_results['language']) + \"\\n\")\n",
    "\n",
    "    i += 1\n",
    "\n",
    "print(\"\\n\\n alaysis terminated\")\n",
    "    \n",
    "nodes_csv.close()\n",
    "nodes_tags_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize as Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh=open('results/' + domain_name + '.txt', 'rb')\n",
    "G=nx.read_edgelist(fh, delimiter='\\t', nodetype = str)\n",
    "fh.close()\n",
    "\n",
    "nx.draw(G, with_labels = False, node_size=2, edge_size = 1)\n",
    "#plt.savefig(\"results/\"+ domain_name + \".png\", format=\"PNG\", dpi = 600)\n",
    "\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
